{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Student Academic Performance Prediction with Gradient Descent\n",
    "This notebook demonstrates predicting student academic performance using our C++ gradient descent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import necessary libraries and load the student dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gradientdescent as gd\n",
    "\n",
    "print(\"Gradient Descent module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "data = pd.read_csv('../../data/dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing\n",
    "Let's explore the dataset and prepare it for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Dataset info:\")\n",
    "data.info()\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum().sum())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(data['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a continuous target variable from academic performance indicators\n",
    "# We'll predict the average grade from semester grades\n",
    "data['avg_grade'] = (data['Curricular units 1st sem (grade)'] + \n",
    "                     data['Curricular units 2nd sem (grade)']) / 2\n",
    "\n",
    "# Remove rows with zero grades (no evaluations)\n",
    "data_clean = data[data['avg_grade'] > 0].copy()\n",
    "\n",
    "print(f\"Clean dataset shape: {data_clean.shape}\")\n",
    "print(f\"Average grade statistics:\")\n",
    "print(data_clean['avg_grade'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for prediction\n",
    "feature_cols = [\n",
    "    'Age at enrollment',\n",
    "    'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Unemployment rate',\n",
    "    'Inflation rate',\n",
    "    'GDP'\n",
    "]\n",
    "\n",
    "X = data_clean[feature_cols].values\n",
    "y = data_clean['avg_grade'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "\n",
    "# Normalize target\n",
    "y_mean = np.mean(y_train)\n",
    "y_std = np.std(y_train)\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "y_test_norm = (y_test - y_mean) / y_std\n",
    "\n",
    "print(f\"Feature means: {X_mean}\")\n",
    "print(f\"Feature stds: {X_std}\")\n",
    "print(f\"Target mean: {y_mean:.4f}, std: {y_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## Model Training with C++ Gradient Descent\n",
    "Train a linear regression model using our C++ implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to the format expected by our C++ code\n",
    "X_train_list = X_train_norm.tolist()\n",
    "y_train_list = y_train_norm.tolist()\n",
    "\n",
    "# Initialize weights with random values\n",
    "np.random.seed(42)\n",
    "n_features = X_train.shape[1]\n",
    "w = [gd.Variable.create(np.random.randn() * 0.1, True) for _ in range(n_features)]\n",
    "b = gd.Variable.create(np.random.randn() * 0.1, True)  # bias term\n",
    "\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Initial weights: {[w_i.value for w_i in w]}\")\n",
    "print(f\"Initial bias: {b.value}\")\n",
    "\n",
    "# Create loss function and optimizer\n",
    "loss_fn = gd.MSE()\n",
    "optimizer = gd.Vanilla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "weights_history = []\n",
    "\n",
    "# Include bias in the weights list for the optimizer\n",
    "all_weights = w + [b]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train one step with all weights including bias\n",
    "    optimizer.train(all_weights, X_train_list, y_train_list, loss_fn, learning_rate)\n",
    "    \n",
    "    # Compute current predictions and loss for monitoring\n",
    "    y_pred = []\n",
    "    for i in range(len(X_train_list)):\n",
    "        pred = gd.Variable.create(0.0)\n",
    "        for j in range(n_features):\n",
    "            x_ij = gd.Variable.create(X_train_list[i][j])\n",
    "            pred = pred + w[j] * x_ij\n",
    "        pred = pred + b  # Add bias\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    loss = loss_fn.compute(y_pred, y_train_list)\n",
    "    losses.append(loss.value)\n",
    "    \n",
    "    # Store current weights\n",
    "    weights_history.append([w_i.value for w_i in w] + [b.value])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.value:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal weights: {[w_i.value for w_i in w]}\")\n",
    "print(f\"Final bias: {b.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "make-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_test_pred_norm = []\n",
    "for i in range(len(X_test_norm)):\n",
    "    pred = b.value  # Start with bias\n",
    "    for j in range(n_features):\n",
    "        pred += w[j].value * X_test_norm[i][j]\n",
    "    y_test_pred_norm.append(pred)\n",
    "\n",
    "# Convert back to original scale\n",
    "y_test_pred = np.array(y_test_pred_norm) * y_std + y_mean\n",
    "\n",
    "# Calculate metrics\n",
    "mse = np.mean((y_test - y_test_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "r2 = 1 - (np.sum((y_test - y_test_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RÂ²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot 1: Prediction Errors (Residuals)\n",
    "plt.subplot(1, 2, 1)\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Grade')\n",
    "plt.ylabel('Prediction Error')\n",
    "plt.title(f'Residuals Plot (RMSE = {rmse:.3f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Feature importance (weights)\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_importance = np.abs([w_i.value for w_i in w])\n",
    "plt.barh(range(len(feature_cols)), feature_importance)\n",
    "plt.yticks(range(len(feature_cols)), [col.replace('Curricular units ', '').replace(' (', '\\n(') for col in feature_cols])\n",
    "plt.xlabel('|Weight|')\n",
    "plt.title('Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-visualization",
   "metadata": {},
   "source": [
    "## Training Visualization\n",
    "Let's visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weights history to numpy array\n",
    "weights_history = np.array(weights_history)\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot weight convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(n_features):\n",
    "    plt.plot(weights_history[:, i], label=f'w[{i}]')\n",
    "plt.plot(weights_history[:, -1], 'k--', label='bias')\n",
    "plt.title('Weight Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature weights\n",
    "print(\"Feature Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (feature, weight) in enumerate(zip(feature_cols, [w_i.value for w_i in w])):\n",
    "    print(f\"{feature:35s}: {weight:8.4f}\")\n",
    "print(f\"{'Bias':35s}: {b.value:8.4f}\")\n",
    "\n",
    "print(\"\\nMost important features (by absolute weight):\")\n",
    "weights_values = [w_i.value for w_i in w]\n",
    "importance_idx = np.argsort(np.abs(weights_values))[::-1]\n",
    "for i in importance_idx[:5]:\n",
    "    print(f\"{feature_cols[i]:35s}: {weights_values[i]:8.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
