{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf87800430b14e9",
   "metadata": {},
   "source": [
    "# Gradient Descent Module Testing Notebook\n",
    "This notebook tests the combined C++ gradient descent library with Python bindings.\n",
    "### Setup\n",
    "Make sure you've built the module using:\n",
    "```bash\n",
    "cd src/notebooks\n",
    "python3 setup.py build_ext --inplace\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b2e3cb9747e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gradientdescent as gd\n",
    "\n",
    "print(\"Gradient Descent module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autodiff_section",
   "metadata": {},
   "source": [
    "## Part 1: Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables\n",
    "x = gd.Variable.create(2.0, True)  # requires_grad=True\n",
    "y = gd.Variable.create(3.0, True)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arithmetic_ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operations\n",
    "z1 = x + y\n",
    "z2 = x * y\n",
    "z3 = x - y\n",
    "z4 = x / y\n",
    "\n",
    "print(f\"x + y = {z1.value}\")\n",
    "print(f\"x * y = {z2.value}\")\n",
    "print(f\"x - y = {z3.value}\")\n",
    "print(f\"x / y = {z4.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple_gradient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient: f(x) = x^2, df/dx = 2x\n",
    "x = gd.Variable.create(3.0, True)\n",
    "f = x * x\n",
    "\n",
    "print(f\"f(x) = x^2 where x = {x.value}\")\n",
    "print(f\"f = {f.value}\")\n",
    "\n",
    "f.backward()\n",
    "print(f\"df/dx = {x.grad} (expected: {2 * x.value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimizer_section",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for linear regression\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# True parameters: w = [2.5, -1.5]\n",
    "true_w = np.array([2.5, -1.5])\n",
    "\n",
    "# Generate random X data\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate y = X * w + noise\n",
    "y = X.dot(true_w) + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "print(f\"Generated {n_samples} samples with {n_features} features\")\n",
    "print(f\"True parameters: {true_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to the format expected by our C++ code\n",
    "X_list = X.tolist()\n",
    "y_list = y.tolist()\n",
    "\n",
    "# Initialize weights with random values\n",
    "w = [gd.Variable.create(np.random.randn(), True) for _ in range(n_features)]\n",
    "print(f\"Initial weights: [{w[0].value}, {w[1].value}]\")\n",
    "\n",
    "# Create loss function and optimizer\n",
    "loss_fn = gd.MSE()\n",
    "optimizer = gd.Vanilla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "weights_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train one step\n",
    "    optimizer.train(w, X_list, y_list, loss_fn, learning_rate)\n",
    "    \n",
    "    # Compute current predictions and loss for monitoring\n",
    "    y_pred = []\n",
    "    for i in range(n_samples):\n",
    "        pred = gd.Variable.create(0.0)\n",
    "        for j in range(n_features):\n",
    "            x_ij = gd.Variable.create(X_list[i][j])\n",
    "            pred = pred + w[j] * x_ij\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    loss = loss_fn.compute(y_pred, y_list)\n",
    "    losses.append(loss.value)\n",
    "    weights_history.append([w[0].value, w[1].value])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.value:.6f}, Weights = [{w[0].value:.4f}, {w[1].value:.4f}]\")\n",
    "\n",
    "print(f\"\\nFinal weights: [{w[0].value:.4f}, {w[1].value:.4f}]\")\n",
    "print(f\"True weights:  [{true_w[0]:.4f}, {true_w[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Results Summary\n",
    "print(\"\\n=== Training Results ===\")\n",
    "print(f\"Final Loss: {losses[-1]:.6f}\")\n",
    "print(f\"Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n",
    "print(f\"\\nWeight Convergence:\")\n",
    "print(f\"w[0]: {w[0].value:.4f} -> target: {true_w[0]:.4f} (error: {abs(w[0].value - true_w[0]):.4f})\")\n",
    "print(f\"w[1]: {w[1].value:.4f} -> target: {true_w[1]:.4f} (error: {abs(w[1].value - true_w[1]):.4f})\")\n",
    "print(f\"\\nLoss progression (every 10 epochs):\")\n",
    "for i in range(0, len(losses), 10):\n",
    "    print(f\"Epoch {i:2d}: {losses[i]:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
