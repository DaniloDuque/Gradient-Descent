{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf87800430b14e9",
   "metadata": {},
   "source": [
    "# Gradient Descent Tutorial\n",
    "This tutorial demonstrates how to use the C++ gradient descent library with Python bindings.\n",
    "### Setup\n",
    "Make sure you've built the module using:\n",
    "```bash\n",
    "cd src/notebooks/\n",
    "python3 setup.py build_ext --inplace\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b2e3cb9747e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gradientdescent as gd\n",
    "\n",
    "print(\"Gradient Descent module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autodiff_section",
   "metadata": {},
   "source": [
    "## Part 1: Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables\n",
    "x = gd.Variable.create(2.0, True)  # requires_grad=True\n",
    "y = gd.Variable.create(3.0, True)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arithmetic_ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operations\n",
    "z1 = x + y\n",
    "z2 = x * y\n",
    "z3 = x - y\n",
    "z4 = x / y\n",
    "\n",
    "print(f\"x + y = {z1.value}\")\n",
    "print(f\"x * y = {z2.value}\")\n",
    "print(f\"x - y = {z3.value}\")\n",
    "print(f\"x / y = {z4.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple_gradient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient: f(x) = x^2, df/dx = 2x\n",
    "x = gd.Variable.create(3.0, True)\n",
    "f = x * x\n",
    "\n",
    "print(f\"f(x) = x^2 where x = {x.value}\")\n",
    "print(f\"f = {f.value}\")\n",
    "\n",
    "f.backward()\n",
    "print(f\"df/dx = {x.grad} (expected: {2 * x.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95beeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex multivariate function using all operations:\n",
    "# f(x,y,z) = exp(x*y) * sin(z) + tanh(x/y) * cos(y^2) + log(x+1) * (z-y)^3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = gd.Variable.create(2.0, True)\n",
    "y = gd.Variable.create(1.5, True) \n",
    "z = gd.Variable.create(0.8, True)\n",
    "\n",
    "# Term 1: exp(x*y) * sin(z)\n",
    "xy = x * y\n",
    "exp_xy = xy.exp()\n",
    "sin_z = z.sin()\n",
    "term1 = exp_xy * sin_z\n",
    "\n",
    "# Term 2: tanh(x/y) * cos(y^2)\n",
    "x_div_y = x / y\n",
    "tanh_term = x_div_y.tanh()\n",
    "y_squared = y.pow(gd.Variable.create(2.0))\n",
    "cos_y2 = y_squared.cos()\n",
    "term2 = tanh_term * cos_y2\n",
    "\n",
    "# Term 3: log(x+1) * (z-y)^3\n",
    "x_plus_1 = x + gd.Variable.create(1.0)\n",
    "log_term = x_plus_1.log()\n",
    "z_minus_y = z - y\n",
    "cube_term = z_minus_y.pow(gd.Variable.create(3.0))\n",
    "term3 = log_term * cube_term\n",
    "\n",
    "# Final function\n",
    "f = term1 + term2 + term3\n",
    "\n",
    "print(f\"f(x={x.value}, y={y.value}, z={z.value}) = {f.value:.6f}\")\n",
    "print(f\"\\nTerm 1 (exp(x*y)*sin(z)): {term1.value:.6f}\")\n",
    "print(f\"Term 2 (tanh(x/y)*cos(y²)): {term2.value:.6f}\")\n",
    "print(f\"Term 3 (log(x+1)*(z-y)³): {term3.value:.6f}\")\n",
    "\n",
    "# Compute gradients\n",
    "f.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"∂f/∂x = {x.grad:.6f}\")\n",
    "print(f\"∂f/∂y = {y.grad:.6f}\")\n",
    "print(f\"∂f/∂z = {z.grad:.6f}\")\n",
    "\n",
    "# Calculate expected values using numpy/math for verification\n",
    "x_val, y_val, z_val = 2.0, 1.5, 0.8\n",
    "\n",
    "# Expected function value\n",
    "exp_term1 = np.exp(x_val * y_val) * np.sin(z_val)\n",
    "exp_term2 = np.tanh(x_val / y_val) * np.cos(y_val**2)\n",
    "exp_term3 = np.log(x_val + 1) * (z_val - y_val)**3\n",
    "exp_f = exp_term1 + exp_term2 + exp_term3\n",
    "\n",
    "# Expected gradients\n",
    "# ∂f/∂x = y*exp(x*y)*sin(z) + (1-tanh²(x/y))*(1/y)*cos(y²) + (1/(x+1))*(z-y)³\n",
    "exp_df_dx = (y_val * np.exp(x_val * y_val) * np.sin(z_val) + \n",
    "             (1 - np.tanh(x_val/y_val)**2) * (1/y_val) * np.cos(y_val**2) + \n",
    "             (1/(x_val+1)) * (z_val-y_val)**3)\n",
    "\n",
    "# ∂f/∂y = x*exp(x*y)*sin(z) - (1-tanh²(x/y))*(x/y²)*cos(y²) - 2*y*tanh(x/y)*sin(y²) - 3*log(x+1)*(z-y)²\n",
    "exp_df_dy = (x_val * np.exp(x_val * y_val) * np.sin(z_val) - \n",
    "             (1 - np.tanh(x_val/y_val)**2) * (x_val/y_val**2) * np.cos(y_val**2) - \n",
    "             2 * y_val * np.tanh(x_val/y_val) * np.sin(y_val**2) - \n",
    "             3 * np.log(x_val+1) * (z_val-y_val)**2)\n",
    "\n",
    "# ∂f/∂z = exp(x*y)*cos(z) + 3*log(x+1)*(z-y)²\n",
    "exp_df_dz = (np.exp(x_val * y_val) * np.cos(z_val) + \n",
    "             3 * np.log(x_val+1) * (z_val-y_val)**2)\n",
    "\n",
    "print(\"\\nExpected values:\")\n",
    "print(f\"f(x,y,z) = {exp_f:.6f}\")\n",
    "print(f\"∂f/∂x = {exp_df_dx:.6f}\")\n",
    "print(f\"∂f/∂y = {exp_df_dy:.6f}\")\n",
    "print(f\"∂f/∂z = {exp_df_dz:.6f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nDifference (autodiff - expected):\")\n",
    "print(f\"f: {f.value - exp_f:.6e}\")\n",
    "print(f\"∂f/∂x: {x.grad - exp_df_dx:.6e}\")\n",
    "print(f\"∂f/∂y: {y.grad - exp_df_dy:.6e}\")\n",
    "print(f\"∂f/∂z: {z.grad - exp_df_dz:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimizer_section",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for linear regression\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# True parameters: w = [2.5, -1.5], bias = 1.0\n",
    "true_w = np.array([2.5, -1.5])\n",
    "true_b = 1.0\n",
    "\n",
    "# Generate random X data\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate y = X * w + bias + noise\n",
    "y = X.dot(true_w) + true_b + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Add bias column to X (column of ones)\n",
    "X_with_bias = np.column_stack([X, np.ones(n_samples)])\n",
    "true_params = np.append(true_w, true_b)\n",
    "\n",
    "print(f\"Generated {n_samples} samples with {n_features} features + bias\")\n",
    "print(f\"True parameters: {true_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to the format expected by our C++ code\n",
    "X_list = X_with_bias.tolist()\n",
    "y_list = y.tolist()\n",
    "\n",
    "# Initialize weights with random values (including bias weight)\n",
    "w = [gd.Variable.create(np.random.randn(), True) for _ in range(n_features + 1)]\n",
    "print(f\"Initial weights: [{w[0].value:.4f}, {w[1].value:.4f}, {w[2].value:.4f}] (last is bias)\")\n",
    "\n",
    "# Create loss function and optimizer\n",
    "loss_fn = gd.MSE()\n",
    "optimizer = gd.Vanilla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 2500\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "weights_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train one step\n",
    "    optimizer.train(w, X_list, y_list, loss_fn, learning_rate)\n",
    "    \n",
    "    # Compute current predictions and loss for monitoring\n",
    "    y_pred = []\n",
    "    for i in range(n_samples):\n",
    "        pred = gd.Variable.create(0.0)\n",
    "        for j in range(n_features + 1):\n",
    "            x_ij = gd.Variable.create(X_list[i][j])\n",
    "            pred = pred + w[j] * x_ij\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    loss = loss_fn.compute(y_pred, y_list)\n",
    "    losses.append(loss.value)\n",
    "    weights_history.append([w[0].value, w[1].value, w[2].value])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.value:.6f}, Weights = [{w[0].value:.4f}, {w[1].value:.4f}, {w[2].value:.4f}]\")\n",
    "\n",
    "print(f\"\\nFinal weights: [{w[0].value:.4f}, {w[1].value:.4f}, {w[2].value:.4f}]\")\n",
    "print(f\"True parameters: [{true_params[0]:.4f}, {true_params[1]:.4f}, {true_params[2]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Results Summary\n",
    "print(\"\\n=== Training Results ===\")\n",
    "print(f\"Final Loss: {losses[-1]:.6f}\")\n",
    "print(f\"Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n",
    "print(f\"\\nParameter Convergence:\")\n",
    "print(f\"w[0]: {w[0].value:.4f} -> target: {true_params[0]:.4f} (error: {abs(w[0].value - true_params[0]):.4f})\")\n",
    "print(f\"w[1]: {w[1].value:.4f} -> target: {true_params[1]:.4f} (error: {abs(w[1].value - true_params[1]):.4f})\")\n",
    "print(f\"bias: {w[2].value:.4f} -> target: {true_params[2]:.4f} (error: {abs(w[2].value - true_params[2]):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plotting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot loss over epochs\n",
    "ax1.plot(losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot weight convergence\n",
    "weights_history = np.array(weights_history)\n",
    "ax2.plot(weights_history[:, 0], 'r-', label=f'w[0] -> {true_w[0]:.2f}', linewidth=2)\n",
    "ax2.plot(weights_history[:, 1], 'g-', label=f'w[1] -> {true_w[1]:.2f}', linewidth=2)\n",
    "ax2.axhline(y=true_w[0], color='r', linestyle='--', alpha=0.7, label='True w[0]')\n",
    "ax2.axhline(y=true_w[1], color='g', linestyle='--', alpha=0.7, label='True w[1]')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Weight Value')\n",
    "ax2.set_title('Weight Convergence')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
