{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7cb9d6",
   "metadata": {},
   "source": [
    "# Amazon Stock Linear Gradient Descent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries and load the Amazon stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gradientdescent as gd\n",
    "\n",
    "print(\"Gradient Descent module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon stock data\n",
    "amazon_data = pd.read_csv('../../data/Amazon.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Let's explore the Amazon stock data to understand its structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset shape:\", amazon_data.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "amazon_data.info()\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "amazon_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-stock-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime format\n",
    "amazon_data['Date'] = pd.to_datetime(amazon_data['Date'])\n",
    "\n",
    "# Plot the closing price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(amazon_data['Date'], amazon_data['Close'], 'b-', linewidth=1)\n",
    "plt.title('Amazon Stock Closing Price (1997-2008)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's prepare the data for our linear regression model. We'll create features based on previous days' closing prices to predict the next day's closing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features: previous n days' closing prices\n",
    "def create_features(data, n_prev_days=5):\n",
    "    \"\"\"Create features from previous days' closing prices.\"\"\"\n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(n_prev_days, len(data)):\n",
    "        # Get previous n days' closing prices as features\n",
    "        prev_prices = data.iloc[i-n_prev_days:i]['Close'].values\n",
    "        features.append(prev_prices)\n",
    "        \n",
    "        # Current day's closing price as target\n",
    "        targets.append(data.iloc[i]['Close'])\n",
    "    \n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "# Create features and targets using 5 previous days\n",
    "n_prev_days = 5\n",
    "X, y = create_features(amazon_data, n_prev_days)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"\\nSample feature (previous {n_prev_days} days' closing prices):\")\n",
    "print(X[0])\n",
    "print(f\"Corresponding target (next day's closing price): {y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-scaling",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Let's normalize our features to improve the convergence of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "\n",
    "# Normalize targets\n",
    "y_mean = np.mean(y_train)\n",
    "y_std = np.std(y_train)\n",
    "\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "y_test_norm = (y_test - y_mean) / y_std\n",
    "\n",
    "print(\"Normalized training features (first sample):\", X_train_norm[0])\n",
    "print(\"Normalized training target (first sample):\", y_train_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-regression",
   "metadata": {},
   "source": [
    "## Linear Regression with Gradient Descent\n",
    "Now, let's implement linear regression using our gradient descent library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to the format expected by our C++ code\n",
    "X_train_list = X_train_norm.tolist()\n",
    "y_train_list = y_train_norm.tolist()\n",
    "\n",
    "# Initialize weights with random values\n",
    "np.random.seed(42)\n",
    "w = [gd.Variable.create(np.random.randn() * 0.1, True) for _ in range(n_prev_days)]\n",
    "b = gd.Variable.create(np.random.randn() * 0.1, True)  # bias term\n",
    "\n",
    "print(f\"Initial weights: {[w_i.value for w_i in w]}\")\n",
    "print(f\"Initial bias: {b.value}\")\n",
    "\n",
    "# Create loss function and optimizer\n",
    "loss_fn = gd.MSE()\n",
    "optimizer = gd.Vanilla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "weights_history = []\n",
    "\n",
    "# Include bias in the weights list for the optimizer\n",
    "all_weights = w + [b]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train one step with all weights including bias\n",
    "    optimizer.train(all_weights, X_train_list, y_train_list, loss_fn, learning_rate)\n",
    "    \n",
    "    # Compute current predictions and loss for monitoring\n",
    "    y_pred = []\n",
    "    for i in range(len(X_train_list)):\n",
    "        pred = gd.Variable.create(0.0)\n",
    "        for j in range(n_prev_days):\n",
    "            x_ij = gd.Variable.create(X_train_list[i][j])\n",
    "            pred = pred + w[j] * x_ij\n",
    "        pred = pred + b  # Add bias\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    loss = loss_fn.compute(y_pred, y_train_list)\n",
    "    losses.append(loss.value)\n",
    "    \n",
    "    # Store current weights\n",
    "    weights_history.append([w_i.value for w_i in w] + [b.value])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.value:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal weights: {[w_i.value for w_i in w]}\")\n",
    "print(f\"Final bias: {b.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-evaluation",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Let's evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_test_pred_norm = []\n",
    "for i in range(len(X_test_norm)):\n",
    "    pred = b.value  # Start with bias\n",
    "    for j in range(n_prev_days):\n",
    "        pred += w[j].value * X_test_norm[i][j]\n",
    "    y_test_pred_norm.append(pred)\n",
    "\n",
    "# Convert normalized predictions back to original scale\n",
    "y_test_pred = np.array(y_test_pred_norm) * y_std + y_mean\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the test set\n",
    "mse = np.mean((y_test - y_test_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get the dates for the test set\n",
    "test_dates = amazon_data['Date'].iloc[train_size + n_prev_days:]\n",
    "\n",
    "plt.plot(test_dates, y_test, 'b-', label='Actual', linewidth=1)\n",
    "plt.plot(test_dates, y_test_pred, 'r-', label='Predicted', linewidth=1)\n",
    "plt.title('Amazon Stock Price: Actual vs Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-visualization",
   "metadata": {},
   "source": [
    "## Training Visualization\n",
    "Let's visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weights history to numpy array\n",
    "weights_history = np.array(weights_history)\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot weight convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(n_prev_days):\n",
    "    plt.plot(weights_history[:, i], label=f'w[{i}]')\n",
    "plt.plot(weights_history[:, -1], 'k--', label='bias')\n",
    "plt.title('Weight Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Let's analyze the importance of each feature (previous days' closing prices) in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final weights\n",
    "final_weights = np.array([w_i.value for w_i in w])\n",
    "\n",
    "# Create labels for each day\n",
    "day_labels = [f'Day -{n_prev_days-i}' for i in range(n_prev_days)]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(day_labels, final_weights)\n",
    "plt.title('Feature Importance: Impact of Previous Days on Stock Price Prediction')\n",
    "plt.xlabel('Previous Days')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the weights\n",
    "print(\"Feature importance (weights):\")\n",
    "for i, label in enumerate(day_labels):\n",
    "    print(f\"{label}: {final_weights[i]:.4f}\")\n",
    "print(f\"Bias: {b.value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
